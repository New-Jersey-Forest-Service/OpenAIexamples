{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQ7YLW9et3My"
   },
   "source": [
    "<font size=\"5\">**OpenAI GPT Basic Example**</font>\n",
    "\n",
    "This is a basic example of how to use the \n",
    "OpenAI GPT API in Python. This Jupyter\n",
    "notebook was designed to run in Google Colab,\n",
    "but can easily work on other platforms as well.\n",
    "\n",
    "In this example we construct a prompt that utilizes\n",
    "a number of variables that describe a forest stand in the \n",
    "formulation of a response.The prompt informs the model of \n",
    "an average value for basal area that it my comment on \n",
    "when drafting the stand description.\n",
    "\n",
    "William Zipse, NJ Forest Service 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nQHPZMBYKv7x"
   },
   "outputs": [],
   "source": [
    "#Set API Key\n",
    "#You will need an OpenAI API key to run this script!\n",
    "#Use your own key where quoted in the API_KEY constant below.\n",
    "#This will establish which key to charge for tokens!\n",
    "#BE CAREFUL!!!\n",
    "API_KEY = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">**Some notes about calling OpenAI from Python**</font>\n",
    "\n",
    "The OpenAI documentation gives examples of loading the API key either for all projects or for a single project. OpenAI recommends using the methods for setting up the key for all projects, setting up an environment variable locally, and calling it in code without including the key. This is not always practical, of course.\n",
    "\n",
    "The other recommended method involves setting up the key for a single project using a .env file that contains the key and calling that file from one's code. The documentation then recommends adding the .env file to .gitignore, if using git version control. This way the key is never included in the code or uploaded to a repository.\n",
    "\n",
    "The code used in this example is meant as a demonstration, so you can put your API key straight into the API_KEY constant listed at the beginning of this file with minimal setup and see how the interface works. NEVER save your key in this example or any code you are working on! If the file is copied or uploaded, you risk publicly exposing a paid API usage key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmUt8hbUB7_O"
   },
   "source": [
    "The code below is used to install the openai Python library\n",
    "in the Google Colab environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12030,
     "status": "ok",
     "timestamp": 1680056764264,
     "user": {
      "displayName": "William Zipse",
      "userId": "05258451565054985349"
     },
     "user_tz": 240
    },
    "id": "vN75wjA4KO4K",
    "outputId": "0efc770a-ce70-4b41-b73b-a77b98fd9599"
   },
   "outputs": [],
   "source": [
    "#install OpenAI\n",
    "#Uncomment below if using Google Colab\n",
    "#comment out if using a local install\n",
    "#!pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "y1Vpq8YLH1I3"
   },
   "outputs": [],
   "source": [
    "#Taken from OpenAI documentation\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BP4SZVFVEM-n"
   },
   "outputs": [],
   "source": [
    "#Establish paths and connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gBVimftEH32h"
   },
   "outputs": [],
   "source": [
    "#Set API Key from API_KEY constant set at the beginning of this notebook\n",
    "client = OpenAI(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vs8r4j0lB7_R"
   },
   "source": [
    "<font size=\"3\">**Variables for the Prompt**</font>\n",
    "\n",
    "The variables below contain the values that will be used in\n",
    "the forest stand description by the GPT model. In this example\n",
    "the values are set in the definitions below, however these \n",
    "variables could easily be changed in other implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Wz1nC-w8JPat"
   },
   "outputs": [],
   "source": [
    "#define forest stand variables\n",
    "qmd = 6.0\n",
    "tpa = 311\n",
    "ba = 95\n",
    "ft = 'Pitch Pine/Oak'\n",
    "area = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oWzWMwKB7_S"
   },
   "source": [
    "<font size=\"3\">**Making the Prompt**</font>\n",
    "\n",
    "The string variable called \"p1\" below contains the prompt to send to the openai API. \n",
    "Note that the variables defined previously are referred to in this prompt. Also \n",
    "note that prompts are preceded by \"Prompt:\" and responses are preceded by \"Response:\" \n",
    "In this case, the response after \"Response:\"is left blank for the model to complete.\n",
    "You may give the model a series of defined prompts and responses prior to leaving a\n",
    "response blank. The delimiters of \"Prompt:\" and \"Response:\" are defined in the call \n",
    "to client.chat.completions.create() in the block after the definition of the variable \"p1\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "D5dRb2s4O2Oo"
   },
   "outputs": [],
   "source": [
    "p1 = '''Prompt:\n",
    "Generate a description of a forest stand where \n",
    "QMD is quadratic mean diameter in inches, TPA is trees per acre, BA is average tree basal \n",
    "area in square feet per acre, FT is forest type naming dominant tree species in the overstory, \n",
    "and area is the stand area in\n",
    "acres. '''+'In this stand QMD = '+str(qmd)+', '+'TPA = '+str(tpa)+', BA = '+str(ba)+', FT = '+str(ft)+'area = '+str(area)+'''\n",
    "The statewide average BA is 111.0 square feet per acre. \\n Response:'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBvtQ0qjB7_T"
   },
   "source": [
    "<font size=\"3\">**Calling the OpenAI API**</font>\n",
    "\n",
    "Below the variable \"response\" is defined by calling the \n",
    "openai.Completion.create() function.  The function returns \n",
    "nested Python dictionaries.  Model parameters can be set here. \n",
    "See OpenAI documentation for details. Note the \"prompt\" variable \n",
    "is set to \"p\" above; max_tokens can be adjusted to adjust the \n",
    "length of prompts and responses (larger values may charge \n",
    "more tokens on the OpenAI account associated with the API key);\n",
    "temperature can be set between 0 - 2. This is were the \"stop\" \n",
    "list is set as well. Here it is set to \"Prompt:\" for the prompt \n",
    "and \"Response:\" for the response.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "87sGWFF8LciJ"
   },
   "outputs": [],
   "source": [
    "#set model parameters here\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a forestry assistant, that explains sceintific concepts as a professional forester.\"},\n",
    "    {\"role\": \"user\", \"content\": p1}\n",
    "  ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kuJ_GavHB7_U"
   },
   "source": [
    "<font size=\"3\">**Locating the Text Response**</font>\n",
    "\n",
    "Below the variable \"answer\" is set to the location of the \n",
    "text response from the \"response\" variable above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jrOQSlAOVxnx"
   },
   "outputs": [],
   "source": [
    "answer = completion.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRXnXV49B7_V"
   },
   "source": [
    "<font size=\"3\">**Let's see the text response!**</font>\n",
    "\n",
    "Here we print the \"answer\" variable generated by the model.\n",
    "\n",
    "Now you can try changing the prompt and variables above and\n",
    "see what kinds of results you can get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 234,
     "status": "ok",
     "timestamp": 1680056806890,
     "user": {
      "displayName": "William Zipse",
      "userId": "05258451565054985349"
     },
     "user_tz": 240
    },
    "id": "i7ZrOZkMY0P5",
    "outputId": "b2e6adfb-0acd-4143-f3e2-6e8269886c37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"In this forest stand, we observe a diverse mix of tree species dominated by Pitch Pine and Oak. The quadratic mean diameter (QMD) of the trees is measured at 6.0 inches, indicating a range of tree sizes within the stand. The stand has a tree density of 311 trees per acre (TPA), indicating a relatively high density of trees. The average tree basal area (BA) is calculated at 95 square feet per acre, slightly below the statewide average of 111.0 square feet per acre.\\n\\nWith an area of 150 acres, this forest stand is a significant tract of land supporting a mix of Pitch Pine and Oak as the dominant tree species. The QMD, TPA, and BA values provide insights into the stand's structure and composition, indicating a varied and dense stand with a mix of tree sizes.\\n\\nOverall, this forest stand with its specific QMD, TPA, and BA values, along with the dominance of Pitch Pine and Oak, contributes to the biodiversity and ecological richness of the area. Understanding these metrics helps foresters like me assess and manage the health and productivity of the forest stand.\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3SHXWnxF3CU"
   },
   "source": [
    "**Let's try using our prompt in a slightly different way.**\n",
    "\n",
    "In the example above we defined one text prompt in the variable, p. This prompt\n",
    "has only one defined prompt after the stop sequence defined with the string\n",
    "of \"Prompt:\". We then follow the prompt with the stop sequence of \"Response:\" and allow the model to generate the response.\n",
    "\n",
    "This format works for relatively small prompts because prompts and responses have limited numbers of tokens (aka characters) that can be processed. It is possible to construct a prompt containing a series of prompts and responses\n",
    "before requesting a response from the model, allowing more tokens to be\n",
    "processed in a prompt sequence.  the prompt stored in the variable \"p2\" is an\n",
    "example of defining a prompt sequence this way.\n",
    "\n",
    "Also note that we setup a function here called askGPT that calls the same type of completion syntax we used earlier, but\n",
    "makes it a little simpler if we have to call multiple completions. the function takes a prompt (p), followed by the \n",
    "description of an assisstant (a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "VBzpw7a7N6T0"
   },
   "outputs": [],
   "source": [
    "#setup a function if you want to pass many prompts\n",
    "#to a model setup the same way for each prompt\n",
    "#the function takes prompt, p and assistant\n",
    "#description a\n",
    "def askGPT(p,a):\n",
    "  response = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=[\n",
    "          {\"role\":\"system\",\"content\":a},\n",
    "          {\"role\":\"user\",\"content\":p}\n",
    "        ]\n",
    "      )\n",
    "  return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_f6m54oN29w"
   },
   "source": [
    "**A prompt with prompts and responses**\n",
    "\n",
    "Notice that although the prompt defined in the variable \"p2\" below prompts the \n",
    "same information as the prompt as that at \"p1,\" this prompt spreads the information across multiple prompts and responses. This allows for the \n",
    "construction of longer prompts with more background information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-MzW4VF4Fx1O"
   },
   "outputs": [],
   "source": [
    "p2 = '''Prompt: What does QMD stand for?\n",
    "Response: QMD is quadratic mean diamer in inches.\\n\n",
    "Prompt: What does TPA stand for?\\n\n",
    "Response: TPA stands for trees per acre.\\n\n",
    "Prompt: What does BA stand for?\\n\n",
    "Response: BA stands for average forest stand basal area in square feet per acre.\\n\n",
    "Prompt: What is the statewide average BA for reference?\\n\n",
    "Response: The statewide average BA is 111.0 square feet per acre.\\n\n",
    "Prompt: What is area?\\n\n",
    "Response: area is the size of the forest stand in acres.\\n\n",
    "Prompt: What is FT?\\n\n",
    "Response: FT is the forest type, which names the dominant tree species in the overstory.\\n\n",
    "Prompt: Generate a description of a forest stand where\n",
    "'''+'in this stand QMD = '+str(qmd)+', '+'TPA = '+str(tpa)+', BA = '+str(ba)+', FT = '+str(ft)+'area = '+str(area)+'''\n",
    "Mention how the stand BA compares to the statewide BA\\n Response:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = \"You are a forestry assistant, that explains sceintific concepts as a professional forester.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "B0bkEwtBMLbE"
   },
   "outputs": [],
   "source": [
    "#call our function to run the model\n",
    "response = askGPT(p2, assistant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "AHDcS3MRMbN4"
   },
   "outputs": [],
   "source": [
    "answer2 = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyZLiCXBO8qN"
   },
   "source": [
    "Once again we have a response that was left blank in p2, generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 249,
     "status": "ok",
     "timestamp": 1680057622940,
     "user": {
      "displayName": "William Zipse",
      "userId": "05258451565054985349"
     },
     "user_tz": 240
    },
    "id": "Gj96E0DPMq1j",
    "outputId": "632082a4-496e-49b9-f993-f4bbf12c7db5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='In this stand, the quadratic mean diameter (QMD) is 6.0 inches, the trees per acre (TPA) is 311, and the average forest stand basal area (BA) is 95 square feet per acre. The dominant tree species in the overstory is Pitch Pine/Oak, and the forest stand covers an area of 150 acres.\\n\\nComparing the stand BA of 95 square feet per acre to the statewide average BA of 111.0 square feet per acre, we can see that this particular stand has a slightly lower basal area than the statewide average. This may indicate a slightly less dense or mature forest stand in comparison to the average across the state.', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "print(answer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5XzcLwCWB7_W"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1r7Jk352C0HS0EofoBbZ13KZbMMaQrn59",
     "timestamp": 1678471776399
    },
    {
     "file_id": "13PjFrznGvIW27HVOok3cPVRfVS8wVxhX",
     "timestamp": 1677864440698
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
