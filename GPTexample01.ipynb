{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQ7YLW9et3My"
   },
   "source": [
    "<font size=\"5\">**OpenAI GPT Basic Example**</font>\n",
    "\n",
    "This is a basic example of how to use the \n",
    "OpenAI GPT API in Python. This Jupyter\n",
    "notebook was designed to run in Google Colab,\n",
    "but can easily work on other platforms as well.\n",
    "\n",
    "In this example we construct a prompt that utilizes\n",
    "a number of variables that describe a forest stand in the \n",
    "formulation of a response.The prompt informs the model of \n",
    "an average value for basal area that it my comment on \n",
    "when drafting the stand description.\n",
    "\n",
    "William Zipse, NJ Forest Service 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQHPZMBYKv7x"
   },
   "outputs": [],
   "source": [
    "#Set API Key\n",
    "#You will need an OpenAI API key to run this script!\n",
    "#Use your own key where quoted in the API_KEY constant below.\n",
    "#This will establish which key to charge for tokens!\n",
    "#BE CAREFUL!!!\n",
    "API_KEY =\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AZ1mZhmdB7_O"
   },
   "outputs": [],
   "source": [
    "#solves problem with openai install in Colab\n",
    "#This is a workaround for a known bug at this time\n",
    "#Uncomment below if using Google Colab and openai won't install\n",
    "#!pip install aiohttp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmUt8hbUB7_O"
   },
   "source": [
    "The code below is used to install the openai Python library\n",
    "in the Google Colab environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12030,
     "status": "ok",
     "timestamp": 1680056764264,
     "user": {
      "displayName": "William Zipse",
      "userId": "05258451565054985349"
     },
     "user_tz": 240
    },
    "id": "vN75wjA4KO4K",
    "outputId": "0efc770a-ce70-4b41-b73b-a77b98fd9599"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting openai\n",
      "  Downloading openai-0.27.2-py3-none-any.whl (70 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 KB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.27.1)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (22.2.0)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.2 yarl-1.8.2\n"
     ]
    }
   ],
   "source": [
    "#install OpenAI\n",
    "#Uncomment below if using Google Colab\n",
    "#comment out if using a local install\n",
    "!pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y1Vpq8YLH1I3"
   },
   "outputs": [],
   "source": [
    "#Taken from OpenAI documentation\n",
    "#os is used if calling environment variables\n",
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BP4SZVFVEM-n"
   },
   "outputs": [],
   "source": [
    "#Establish paths and connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QH4aX9mdKcN9"
   },
   "outputs": [],
   "source": [
    "#This code is to set the API Key through environment variables\n",
    "#uncomment below if using API key through environment varibles method only!\n",
    "#!export OPENAI_API_KEY=\"<OPENAI_API_KEY>\"\n",
    "#openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gBVimftEH32h"
   },
   "outputs": [],
   "source": [
    "#Set API Key from API_KEY constant set at the beginning of this notebook\n",
    "openai.api_key = API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vs8r4j0lB7_R"
   },
   "source": [
    "<font size=\"3\">**Variables for the Prompt**</font>\n",
    "\n",
    "The variables below contain the values that will be used in\n",
    "the forest stand description by the GPT model. In this example\n",
    "the values are set in the definitions below, however these \n",
    "variables could easily be changed in other implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wz1nC-w8JPat"
   },
   "outputs": [],
   "source": [
    "#define forest stand variables\n",
    "qmd = 6.0\n",
    "tpa = 311\n",
    "ba = 95\n",
    "ft = 'Pitch Pine/Oak'\n",
    "area = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oWzWMwKB7_S"
   },
   "source": [
    "<font size=\"3\">**Making the Prompt**</font>\n",
    "\n",
    "The string variable called \"p\" below contains the prompt to send to the openai API. \n",
    "Note that the variables defined previously are referred to in this prompt. Also \n",
    "note that prompts are preceded by \"Prompt:\" and responses are preceded by \"Response:\" \n",
    "In this case, the response after \"Response:\"is left blank for the model to complete.\n",
    "You may give the model a series of defined prompts and responses prior to leaving a\n",
    "response blank. The delimiters of \"Prompt:\" and \"Response:\" are defined in the call \n",
    "to openai.Completion.create() in the block after the definition of the variable \"p\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5dRb2s4O2Oo"
   },
   "outputs": [],
   "source": [
    "p1 = '''Prompt:\n",
    "Generate a description of a forest stand where \n",
    "QMD is quadratic mean diameter in inches, TPA is trees per acre, BA is average tree basal \n",
    "area in square feet per acre, FT is forest type naming dominant tree species in the overstory, \n",
    "and area is the stand area in\n",
    "acres. '''+'In this stand QMD = '+str(qmd)+', '+'TPA = '+str(tpa)+', BA = '+str(ba)+', FT = '+str(ft)+'area = '+str(area)+'''\n",
    "The statewide average BA is 111.0 square feet per acre. \\n Response:'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBvtQ0qjB7_T"
   },
   "source": [
    "<font size=\"3\">**Calling the OpenAI API**</font>\n",
    "\n",
    "Below the variable \"response\" is defined by calling the \n",
    "openai.Completion.create() function.  The function returns \n",
    "nested Python dictionaries.  Model parameters can be set here. \n",
    "See OpenAI documentation for details. Note the \"prompt\" variable \n",
    "is set to \"p\" above; max_tokens can be adjusted to adjust the \n",
    "length of prompts and responses (larger values may charge \n",
    "more tokens on the OpenAI account associated with the API key);\n",
    "temperature can be set between 0 - 2. This is were the \"stop\" \n",
    "list is set as well. Here it is set to \"Prompt:\" for the prompt \n",
    "and \"Response:\" for the response.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87sGWFF8LciJ"
   },
   "outputs": [],
   "source": [
    "#set model parameters here\n",
    "response = openai.Completion.create(\n",
    "  model=\"text-davinci-003\",\n",
    "  prompt= str(p1),\n",
    "  temperature=1.1,\n",
    "  max_tokens=200,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0,\n",
    "  stop=[\"Prompt:\", \"Response:\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kuJ_GavHB7_U"
   },
   "source": [
    "<font size=\"3\">**Locating the Text Response**</font>\n",
    "\n",
    "Below the variable \"answer\" is set to the location of the \n",
    "text response from the \"response\" variable above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jrOQSlAOVxnx"
   },
   "outputs": [],
   "source": [
    "answer = response['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRXnXV49B7_V"
   },
   "source": [
    "<font size=\"3\">**Let's see the text response!**</font>\n",
    "\n",
    "Here we print the \"answer\" variable generated by the model.\n",
    "\n",
    "Now you can try changing the prompt and variables above and\n",
    "see what kinds of results you can get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 234,
     "status": "ok",
     "timestamp": 1680056806890,
     "user": {
      "displayName": "William Zipse",
      "userId": "05258451565054985349"
     },
     "user_tz": 240
    },
    "id": "i7ZrOZkMY0P5",
    "outputId": "b2e6adfb-0acd-4143-f3e2-6e8269886c37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This forest stand of Pitch Pine/Oak consists of an area of 150 acres, with an average QMD of 6.0 inches, 311 trees per acre and 95 square feet of basal area per acre. The overall basal area in this stand is below the statewide average of 111, making it one of the lower-density stands in the region.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3SHXWnxF3CU"
   },
   "source": [
    "**Let's try using our prompt in a slightly different way.**\n",
    "\n",
    "In the example above we defined one text prompt in the variable, p. This prompt\n",
    "has only one defined prompt after the stop sequence defined with the string\n",
    "of \"Prompt:\". We then follow the prompt with the stop sequence of \"Response:\" and allow the model to generate the response.\n",
    "\n",
    "This format works for relatively small prompts because prompts and responses have limited numbers of tokens (aka characters) that can be processed. It is possible to construct a prompt containing a series of prompts and responses\n",
    "before requesting a response from the model, allowing more tokens to be\n",
    "processed in a prompt sequence.  the prompt stored in the variable \"p2\" is an\n",
    "example of defining a prompt sequence this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VBzpw7a7N6T0"
   },
   "outputs": [],
   "source": [
    "#setup a function if you want to pass many prompts\n",
    "#to a model setup the same way for each prompt\n",
    "def askGPT(p):\n",
    "  response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt= str(p),\n",
    "    temperature=1.1,\n",
    "    max_tokens=200,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0.0,\n",
    "    presence_penalty=0.0,\n",
    "    stop=[\"Prompt:\", \"Response:\"])\n",
    "  return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_f6m54oN29w"
   },
   "source": [
    "**A prompt with prompts and responses**\n",
    "\n",
    "Notice that although the prompt defined in the variable \"p2\" below prompts the \n",
    "same information as the prompt as that at \"p1,\" this prompt spreads the information across multiple prompts and responses. This allows for the \n",
    "construction of longer prompts with more background information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-MzW4VF4Fx1O"
   },
   "outputs": [],
   "source": [
    "p2 = '''Prompt: What does QMD stand for?\n",
    "Response: QMD is quadratic mean diamer in inches.\\n\n",
    "Prompt: What does TPA stand for?\\n\n",
    "Response: TPA stands for trees per acre.\\n\n",
    "Prompt: What does BA stand for?\\n\n",
    "Response: BA stands for average forest stand basal area in square feet per acre.\\n\n",
    "Prompt: What is the statewide average BA for reference?\\n\n",
    "Response: The statewide average BA is 111.0 square feet per acre.\\n\n",
    "Prompt: What is area?\\n\n",
    "Response: area is the size of the forest stand in acres.\\n\n",
    "Prompt: What is FT?\\n\n",
    "Response: FT is the forest type, which names the dominant tree species in the overstory.\\n\n",
    "Prompt: Generate a description of a forest stand where\n",
    "'''+'in this stand QMD = '+str(qmd)+', '+'TPA = '+str(tpa)+', BA = '+str(ba)+', FT = '+str(ft)+'area = '+str(area)+'''\n",
    "Mention how the stand BA compares to the statewide BA\\n Response:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0bkEwtBMLbE"
   },
   "outputs": [],
   "source": [
    "#call our function to run the model\n",
    "response = askGPT(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AHDcS3MRMbN4"
   },
   "outputs": [],
   "source": [
    "answer2 = response['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyZLiCXBO8qN"
   },
   "source": [
    "Once again we have a response that was left blank in p2, generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 249,
     "status": "ok",
     "timestamp": 1680057622940,
     "user": {
      "displayName": "William Zipse",
      "userId": "05258451565054985349"
     },
     "user_tz": 240
    },
    "id": "Gj96E0DPMq1j",
    "outputId": "632082a4-496e-49b9-f993-f4bbf12c7db5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This stand is 150 acres in size, with a Quadratic Mean Diameter (QMD) of 6.0 inches and Trees Per Acre (TPA) of 311. It is a Pitch Pine/Oak forest type with an Average Forest Stand Basal Area (BA) of 95 square feet per acre, which is 14.0 square feet per acre lower than the statewide average.\n"
     ]
    }
   ],
   "source": [
    "print(answer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5XzcLwCWB7_W"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1r7Jk352C0HS0EofoBbZ13KZbMMaQrn59",
     "timestamp": 1678471776399
    },
    {
     "file_id": "13PjFrznGvIW27HVOok3cPVRfVS8wVxhX",
     "timestamp": 1677864440698
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
